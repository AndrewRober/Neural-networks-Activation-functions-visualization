# Activation Function Plots

This repository contains a Python script to plot various activation functions used in neural networks using TensorFlow and Matplotlib. The purpose of this code is to visualize the different activation functions, their shapes, and how they transform input data. By comparing the plots, you can get a better understanding of the properties of each activation function and their potential impact on the training and performance of neural networks.

## Activation Functions

The script plots the following activation functions:

1. Linear
2. Rectified Linear Unit (ReLU)
3. Leaky Rectified Linear Unit (Leaky ReLU)
4. Parameterized Rectified Linear Unit (PReLU)
5. Exponential Linear Unit (ELU)
6. Scaled Exponential Linear Unit (SELU)
7. Gaussian Error Linear Unit (GELU)
8. Sigmoid
9. Hyperbolic Tangent (Tanh)
10. Hard Sigmoid
11. Softmax
12. Softplus
13. Softsign
14. Swish
15. Exponential

![download](https://github.com/AndrewRober/Neural-networks-Activation-functions-visualization/assets/54873972/5d0b800d-f675-4b25-8d32-be047e66ef6d)

## Requirements

- Python 3.6 or higher
- TensorFlow 2.x
- Matplotlib
- NumPy
